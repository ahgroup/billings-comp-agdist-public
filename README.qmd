---
format: gfm
---

<!-- README.md is generated from README.qmd. Please edit that file -->

```{r setup, include = FALSE}
require(rmarkdown, quietly = TRUE)
require(yaml, quietly = TRUE)
require(devtools, quietly = TRUE)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# CompAgDist (CAD)

<!-- badges: start -->
<!-- badges: end -->

This project, tentatively titled "Impact of antigenic distance on the heterologous immune response to seasonal influenza vaccine in an elderly human cohort" focuses on analyzing the UGAFluVac dataset. This is a followup to [this research project](https://github.com/ahgroup/SD-HD-flu-vaccine), which analyzed heterologous immunity separately for each vaccine strain that was analyzed in the UGAFluVac data. For this project, we plan to build on our previous work on antigenic distances, and replace discrete strain-specific effects with the continuous effect of antigenic distance.

We will control for confounding in a causal analysis framework and used bayesian multilevel modeling to estimate the impact of antigenic distance on vaccine outcomes, specifically focusing on comparing individuals who received HD and SD vaccines.

## Software requirements

- `r version$version.string`
- Quarto version `r quarto::quarto_version()`
- Cmdstan version `r cmdstanr::cmdstan_version()`
- Several R packages which are listed in `renv.lock` and in the system output shown below.
- The listed dependencies for all of those things
- Note that the following packages *must* be built from source if you run this script on UGA's sapelo2 cluster. The project `runner.R` script called by `job.sh` should? take care of this. (The R version on the cluster does not provide the necessary internal LAPACK libraries but the compiler can link to the correct LAPACK setup on the cluster. For some reason there appears to be no option to force `renv` to compile specific (or even all) packages from source during `renv::restore()`. I tried setting the option `pkgType` which `renv` claims to support, but actually does not seem to do anything.)
    - `RcppArmadillo@14.2.2-1`
    - `mvtnorm@1.3-2`
    - `nloptr@2.1.1`
    - `igraph@2.1.2`
- On Windows, you'll need Rtools 44 (LINK HERE). If you're on MacOS or any kind of Linux or any other OS, we can't guarantee that our code will work, and you'll need to make sure you have any necessary system dependencies.

## Running the project

To run this project on a Slurm cluster, you should log on to your Slurm cluster account. Then you can clone this project from GitHub however you want (I recommend using `ssh` but you don't have to).

For other handelgroup members, I recommend the following steps.

1. Log into the cluster using `ssh your_my_id@sapelo2.gacrc.uga.edu`.
1. Set up an ssh key in your home directory following [GitHub's directions](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent) (you can just use all the default settings).
1. Clone the git repo into your home directory (or scratch, our jobs are small enough that it really shouldn't matter although scratch is technically preferred since we'll be running code and I/O stuff, if your project will generate over 200GB of files you MUST use scratch).
1. Start an interactive job, request at least 10 GB of memory (you can cancel it and get more if you need to).
1. In the interactive job, load R and start an interactive R session. Install all of your dependencies. Note that anything that references LAPACK will need to be built from source which renv might mess up if you just run `renv::restore()`. Also make sure you install cmdstanr for this project.
1. Now you can run the actual cluster job, that will be run like this:

`sbatch --mail-user=email@email.com job.sh`

Of course you should put your email in there, not a fake one. You can hardcode your email in the `job.sh` script but you probably shouldn't since this code will be public and people might (though unlikely) run your code in the future without changing anything and you will get emails.

## Repository structure and content

The template comes with a folder structure and example files to illustrative the kinds of content you would place in the different folders. The following is a brief description of the contents. See the `readme` files in each folder for more details.


* The `assets` folder contains static assets like manually generated schematics/diagrams, bibtex files, csl style files, PDFs of references, and other such content. These assets are not code-based and are not generated by code. Basically add anything that's you want to be part of this repo but that doesn't fit into the other categories.

* All code goes into the `code` folder and subfolders. Currently, there are 3 sub-folders that do different parts of an analysis. You can re-organize such that it makes most sense for your project. The folders contain files that do some data cleaning and analysis to illustrate the overall setup and workflow. See the readme files in those folders for details.

* All data goes into the `data` folder and subfolders. Currently, there are 2 sub-folders that contain different versions of a simple example data set. You can re-organize such that it makes most sense for your project. 

* The `products` folder and its subfolders contains deliverables, such as manuscript/report, the supplement, slide decks, posters, Shiny web apps, etc. Those should generally be made with Quarto/R. As needed, other formats can be used. There is an example manuscript and and example slide deck.
  - The  `manuscript` subfolder contains a template for a report written as Quarto file. If you access this repository as part of [my Modern Applied Data Science course](https://andreashandel.github.io/MADAcourse/), the sections are guides for your project. If you found your way to this repository outside the course, you might only be interested in seeing how the file pulls in results and references and generates a word document as output, without paying attention to the detailed structure. There is also a sub-folder containing an example template for a supplementary material file.
  - The `slides` subfolder contains a basic example of slides made with Quarto.


* The `results` folder contains automatically/code generated output. This includes figures, tables saved as serialized R data (`.Rds`) files, computed values and other outputs. All content in these folders should be automatically generated by code. Manually generated results should be avoided as much as possible. If absolutely necessary, they go into the `assets` folder.


* There are multiple special files in the repo.
  * `readme.md`: this file contains instructions or details about the folder it
  is located in. You are reading the project-level `README.md` file right now. There is a `readme` in almost every folder.
  * `data-analysis-template.Rproj` is a file that tells RStudio that this is the main folder for a project. Rename if you want.
  * a few "hidden" files and folders (they start with a `.` and depending on how your OS is configured, you might not see them). Those are for R/RStudio and Git/GitHub and you can ignore them.

## Endpoint files

```{r}
#| label: endpoint file paths

print_paths <- function(paths) {paste0('`', paths, '`', collapse = "; ")}

# Make sure all "endpoint" files are loadable.
# Mostly cause it makes the pipeline plot look nice.
# Manuscript and supplement and metadata
ms_files <- targets::tar_read("manuscript")
sp_files <- targets::tar_read("supplement")
md_files <- targets::tar_read("zipped_metadata_submission")
# Data files
reporting_data_files <- targets::tar_read("civics_reporting_data_files")
model_data_files <- targets::tar_read("model_data_files")
# Model files
model_metadata <- targets::tar_read("model_metadata_file")
prior_fits <- targets::tar_read("brms_prior_fit_files")
posterior_fits <- targets::tar_read("brms_posterior_fit_files")
```

In addition to our figures and tables for the manuscript, our pipeline produces multiple output files which could be suitable for further examination or analysis. These are included in our repo as the following files.

* Main manuscript: `r print_paths(ms_files[1:2])`.
* Supplementary material: `r print_paths(sp_files[1:2])`.
* Auxiliary files for generating manuscript and supplement: `r unique(c(print_paths(ms_files[-c(1:2)]), print_paths(sp_files[-c(1:2)])))`.
* Zipped NIH CIVICs metadata files: `r print_paths(md_files)`.
* NIH CIVICs-standard formatted dataset: `r print_paths(reporting_data_files)`.
* Transformed dataset for modeling: `r print_paths(model_data_files)`.
* Model metadata for ordering the model fits: `r print_paths(model_metadata)`
* Prior samples from models (directory): `r prior_fits |> dirname() |> unique() |> print_paths()`.
* Posterior sampels from models (directory): `r posterior_fits |> dirname() |> unique() |> print_paths()`.

## License

All of the code we wrote is distributed under the [Affero GNU GPL v3.0](LICENSE.md). Any code that was not written by us has been attributed and is released under the conditions of its own license. Any text and images we created are licensed under the [Creative Commons 4.0 BY-NC-SA international license](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en).

## TODO

- [x] Finish targetization
    - [x] targetize supplement results
    - [x] targetize model comparison
    - [x] targetize model results
- [x] typo in table demographics
- [x] Make sure model description in manuscript matches final models
- [x] Decide whether and when to mention UGA 2016 starting in Winter
- [x] Change UGA labelling to GA throughout (decided not to)
- [x] IRR to main text
- [x] Correlations calculations and table for supplement
- [x] Table of model explanations
- [x] Move ELPD table to before LMM table
- [x] CIVICs reporting data
- [x] Cleanup unnecessary files and targets
    - [x] Remove gap sd bootstrap file target
    - [x] Remove ICC summary file target
    - [x] Remove old files to aux
    - [x] Make sure all results are generated by pipeline
- [x] Finish supplement skeleton
    - [x] Other metrics analysis
    - [x] WAIC analysis (not doing this)
    - [x] Vaccine-specific predictions analysis
    - [x] Misc todos
    - [x] Methods writing
    - [x] update models in supplement
- [ ] Update main text methods
- [ ] Rewrites after final models run
    - [ ] Consider revising small/low correlation language
    - [ ] Simplify "predictions made by ...." paragraph
        - [ ] explain high and low slopes
        - [x] **directly compare ELPDs across the four models**
    - [ ] Discussion revisions
- [ ] reproducibility writing
    - [ ] create public repo
    - [ ] write reproducibility instructions
    - [ ] archive on zenodo
    - [ ] add zenodo and GH text in manuscript
    - [ ] Finish README
- [ ] Final review
    - [ ] Change B lineage mentions to / notation (REVIEW ALL PLOTS/TABLES)
    - [ ] Review all plots tables for order of metrics
    - [ ] Add supplement chapters and stuff to MS

## Session info

```{r}
#| label: sessioninfo
#| message: false
#| warning: false
#| echo: false
deps <-
	renv::dependencies(quiet = TRUE)$Package |>
	unique() |>
	sort()
out <-
	lapply(
		deps[!deps == "box"],
		FUN = \(x, ...) suppressPackageStartupMessages(library(x, ...)),
		character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE
	)
sessionInfo()
```


<!-- END OF FILE -->
